{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add ViT code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/miniconda3/envs/netvlad/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "\n",
    "# Load pretrained ViT model\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "# image = Image.open(\"/home/ds/Research/lcd/orbslam3_docker/Datasets/KITTI/data_odometry_gray/dataset/sequences/00/image_0/000000.png\")\n",
    "image = Image.open(\"checkboard_7.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the CLS token as the place descriptor\n",
    "place_descriptor = outputs.last_hidden_state[:, 0, :]  # Shape: (1, 768)\n",
    "print(place_descriptor.shape)  # (1, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0481, -0.2065,  0.0631,  0.0353,  0.1081, -0.2221,  0.0036,  0.1943,\n",
       "         -0.0445, -0.3849, -0.1312,  0.3608,  0.2188,  0.2202, -0.2179, -0.1992,\n",
       "          0.0409, -0.3344, -0.0285, -0.1340,  0.1087, -0.3145, -0.1056,  0.2532,\n",
       "          0.0587, -0.2087, -0.1091,  0.0106, -0.1033,  0.0574,  0.1291,  0.3898,\n",
       "          0.1081,  0.2517, -0.0446, -0.3196, -0.0538,  0.2293,  0.2498, -0.2004,\n",
       "          0.2402,  0.0850,  0.0926, -0.3626, -0.1459,  0.1840, -0.0821, -0.4395,\n",
       "          0.0420,  0.0928, -0.0443,  0.0791,  0.1611, -0.2228, -0.0389,  0.0117,\n",
       "         -0.1011,  0.0681,  0.1490,  0.0162,  0.0156, -0.0220,  0.5471, -0.1246,\n",
       "          0.0195, -0.1269,  0.2465, -0.1130, -0.3081, -0.4494,  0.2540, -0.0963,\n",
       "         -0.1244,  0.0351,  0.0513,  0.1245, -0.0142, -0.1594, -0.0404,  0.1563,\n",
       "          0.0183, -0.1800,  0.0819,  0.2815, -0.2191,  0.1200,  0.1453,  0.0298,\n",
       "         -0.0651, -0.1544, -0.0502,  0.2575,  0.2558, -0.1156, -0.0203, -0.2655,\n",
       "          0.3792, -0.2572,  0.4494, -0.0838, -0.2593,  0.1635, -0.1870, -0.3683,\n",
       "         -0.1454, -0.1191,  0.0279, -0.0122,  0.0650,  0.0212, -0.0878, -0.3274,\n",
       "         -0.5437, -0.0144,  0.4289, -0.0650, -0.0076,  0.0824,  0.2098, -0.1649,\n",
       "         -0.0151, -0.0174,  0.2416,  0.0710,  0.0617,  0.0360, -0.0665,  0.0998,\n",
       "          0.0393, -0.1930, -0.3299, -0.1547,  0.2384, -0.1471,  0.3501,  0.0724,\n",
       "         -0.1639, -0.4863,  0.1101, -0.0552, -0.4315, -0.2868, -0.4501, -0.2117,\n",
       "          0.3370, -0.0323,  0.2319, -0.1126,  0.1770,  0.1551, -0.0161,  0.0333,\n",
       "          0.1585,  0.1407,  0.0182, -0.2337,  0.1757, -0.1072, -0.1794, -0.3247,\n",
       "          0.3070, -0.0322, -0.2229, -0.2158,  0.2242,  0.2878, -0.1372,  0.1202,\n",
       "          0.1725, -0.4055,  0.2365,  0.0625,  0.1516, -0.2920,  0.4789, -0.1636,\n",
       "         -0.2146, -0.1701,  0.1177,  0.5122,  0.0657, -0.0428,  0.1116,  0.0953,\n",
       "         -0.1632, -0.0635, -0.1495,  0.2409, -0.1047, -0.2162,  0.2031,  0.2238,\n",
       "         -0.1095, -0.2292,  0.1149, -0.2104, -0.0704, -0.0565, -0.0320,  0.1934,\n",
       "         -0.1946, -0.1235,  0.1688,  0.0221, -0.5247,  0.1374, -0.0732, -0.0601,\n",
       "          0.1534,  0.0327,  0.1708, -0.4671,  0.0293, -0.0589,  0.0112, -0.1830,\n",
       "          0.0134, -0.1453,  0.4329,  0.3652, -0.3112,  0.0525, -0.1548, -0.0037,\n",
       "          0.2396,  0.0971,  0.0013,  0.0589,  0.1510, -0.2784, -0.0244,  0.1174,\n",
       "          0.0343, -0.1695, -0.2000,  0.0846, -0.0922, -0.0330,  0.0410,  0.1098,\n",
       "         -0.0429,  0.1807, -0.0260, -0.0512, -0.1165,  0.1320, -0.1122, -0.0249,\n",
       "         -0.1623,  0.1146,  0.0699, -0.0316,  0.2725, -0.0205,  0.2848, -0.0006,\n",
       "          0.2807, -0.0386, -0.1530,  0.2963, -0.3799, -0.4219, -0.3740,  0.0088,\n",
       "          0.1125, -0.2864,  0.2468,  0.2064,  0.1692,  0.4018, -0.0245, -0.2081,\n",
       "         -0.0381,  0.0840, -0.0808,  0.2363, -0.2808,  0.2555,  0.1490,  0.2144,\n",
       "          0.0620,  0.1927,  0.1105,  0.0458, -0.0557,  0.4379, -0.3554,  0.2764,\n",
       "         -0.1554,  0.1388, -0.1980,  0.0049,  0.0021,  0.2287, -0.2965,  0.0785,\n",
       "          0.0220, -0.2304,  0.4288, -0.1661, -0.0222,  0.2421,  0.0012,  0.0602,\n",
       "         -0.2261, -0.2228, -0.0811, -0.1235,  0.2201,  0.0433,  0.0775, -0.1928,\n",
       "         -0.2864, -0.1271, -0.0901, -0.0398, -0.1341,  0.3796, -0.0902, -0.1759,\n",
       "         -0.2223, -0.4061,  0.0718, -0.4846, -0.2013, -0.4190,  0.1089, -0.1250,\n",
       "         -0.0256, -0.0673, -0.2655, -0.2030,  0.0524, -0.2315, -0.1242, -0.0743,\n",
       "          0.1518,  0.3579, -0.1124,  0.0072,  0.2506,  0.0093, -0.1723, -0.0458,\n",
       "         -0.0327,  0.0494, -0.1558, -0.0241, -0.0287, -0.4276, -0.1656,  0.2390,\n",
       "         -0.1967,  0.0818, -0.2234,  0.2116,  0.0434, -0.4749, -0.0614, -0.1480,\n",
       "         -0.0448,  0.0137,  0.0398, -0.2142, -0.0274, -0.2853,  0.3199,  0.1155,\n",
       "         -0.2256, -0.3828, -0.0345, -0.4505,  0.3082,  0.0663, -0.0495, -0.1004,\n",
       "         -0.1173,  0.2855, -0.2853, -0.0015,  0.3615,  0.5459, -0.1405, -0.2476,\n",
       "         -0.0180,  0.1117,  0.4177,  0.2737,  0.0813,  0.0665,  0.0092,  0.1866,\n",
       "          0.1440, -0.2096, -0.3674,  0.1725, -0.2322, -0.0093,  0.0048, -0.3216,\n",
       "          0.0742,  0.0206,  0.0977, -0.1613, -0.0053,  0.3608,  0.1536,  0.0882,\n",
       "          0.0421, -0.4721, -0.1654,  0.1160, -0.1009,  0.2318, -0.3988, -0.2549,\n",
       "         -0.1448, -0.0313, -0.0781, -0.1816, -0.3143, -0.3039,  0.2008, -0.3171,\n",
       "          0.0253,  0.1108,  0.1696,  0.0338, -0.0636,  0.2085, -0.1063, -0.4135,\n",
       "          0.0059, -0.4588, -0.0867,  0.1724, -0.0615, -0.1721,  0.3149,  0.2072,\n",
       "          0.0887, -0.0302, -0.1452, -0.2642, -0.0486,  0.0070, -0.1726, -0.2730,\n",
       "          0.5344,  0.0205,  0.0361,  0.2325,  0.3013, -0.1194,  0.1295, -0.0983,\n",
       "          0.1599,  0.1995, -0.0892, -0.1716, -0.4022, -0.1109,  0.0300,  0.0363,\n",
       "          0.2133,  0.1642, -0.0142,  0.2660, -0.3387,  0.0590,  0.0795, -0.2964,\n",
       "          0.1099,  0.1651, -0.1631,  0.0201, -0.2736, -0.4877,  0.1761, -0.1441,\n",
       "          0.0915, -0.0815, -0.6376,  0.2786, -0.1530, -0.4628,  0.0919, -0.2628,\n",
       "         -0.0689,  0.2059, -0.0912, -0.2433, -0.0081,  0.1768, -0.2066, -0.1811,\n",
       "         -0.3550,  0.2254, -0.2138, -0.1922, -0.1460,  0.3011,  0.0820, -0.0092,\n",
       "         -0.0810, -0.1666, -0.0367, -0.2311,  0.1829, -0.0750,  0.1773, -0.0292,\n",
       "          0.0428, -0.0322, -0.0716,  0.2984, -0.1562, -0.3113, -0.0097,  0.0061,\n",
       "         -0.2279,  0.0335, -0.0991, -0.2010, -0.2068, -0.0409, -0.1290, -0.1702,\n",
       "         -0.1230,  0.1156, -0.3158,  0.0731,  0.4032,  0.0547, -0.2190,  0.1650,\n",
       "         -0.0315, -0.1570,  0.2540, -0.4791,  0.2458, -0.1714, -0.0771,  0.0676,\n",
       "         -0.3475, -0.4712,  0.0817, -0.0837, -0.5259,  0.4747, -0.3008, -0.2199,\n",
       "         -0.1206, -0.0292,  0.2264,  0.3031,  0.1159,  0.2500,  0.0330, -0.2312,\n",
       "          0.1269,  0.1300,  0.4459, -0.0812, -0.1933, -0.4255, -0.0863,  0.0062,\n",
       "          0.1758, -0.1879,  0.2678,  0.0346, -0.0092, -0.1489, -0.2017, -0.1553,\n",
       "          0.0064, -0.0440,  0.3678,  0.4572, -0.1318, -0.1931, -0.2717, -0.4012,\n",
       "         -0.1666,  0.3065,  0.2767,  0.4581,  0.2811, -0.1858,  0.1472, -0.1927,\n",
       "          0.3083,  0.2633, -0.0802,  0.4307, -0.2202, -0.1404, -0.3281, -0.1416,\n",
       "         -0.0865,  0.1370,  0.1501, -0.2852,  0.0664, -0.0542, -0.0389,  0.2489,\n",
       "          0.0162,  0.0752, -0.1293, -0.0037, -0.0167,  0.1016,  0.2728, -0.4318,\n",
       "         -0.3808, -0.1486, -0.2825, -0.0455,  0.0400,  0.0554,  0.0706,  0.0817,\n",
       "         -0.3936, -0.1559,  0.1721,  0.0819,  0.2000, -0.2941,  0.1642,  0.4434,\n",
       "          0.3339, -0.0642,  0.1968, -0.1377, -0.0441, -0.3478, -0.0558, -0.0881,\n",
       "          0.0709, -0.1519, -0.2195, -0.3515,  0.2492, -0.0081,  0.1885, -0.2479,\n",
       "         -0.1145, -0.1813,  0.1241,  0.0760,  0.0657,  0.3053,  0.1210, -0.1914,\n",
       "         -0.0876, -0.0402, -0.1057,  0.3089,  0.2217,  0.0887, -0.1132, -0.1328,\n",
       "          0.0486, -0.2095,  0.0520, -0.2553, -0.0992, -0.2150,  0.1222, -0.0937,\n",
       "          0.0394, -0.0147,  0.2745, -0.2351,  0.1580,  0.3265, -0.1009,  0.0567,\n",
       "         -0.0464, -0.1009,  0.1882,  0.0525, -0.0828, -0.2073, -0.0682,  0.3333,\n",
       "          0.0966, -0.0879,  0.4005, -0.1831, -0.0839, -0.2317,  0.1339,  0.1016,\n",
       "         -0.2243,  0.0938, -0.0933, -0.1209,  0.1159, -0.1386, -0.0206, -0.1508,\n",
       "         -0.1015, -0.1236,  0.0493, -0.1344,  0.2354,  0.0586, -0.0941, -0.0412,\n",
       "          0.2645, -0.1941,  0.2057, -0.2401, -0.0973, -0.1817, -0.0925, -0.0034,\n",
       "         -0.1130, -0.1939, -0.1913, -0.4245, -0.0217,  0.0552,  0.0063,  0.1130,\n",
       "          0.0061, -0.0646, -0.3539,  0.2130,  0.5273,  0.0495, -0.1594, -0.1946,\n",
       "         -0.2993, -0.3717, -0.0356, -0.1410, -0.2888,  0.3015,  0.0722,  0.1017,\n",
       "          0.0258,  0.0631,  0.1827, -0.0233,  0.1000, -0.2901,  0.1027,  0.1426,\n",
       "         -0.2963,  0.1076,  0.1350,  0.1806,  0.0299, -0.1902,  0.0079,  0.1204,\n",
       "          0.2310,  0.1850,  0.0814,  0.1357, -0.2453,  0.1139, -0.0148, -0.1291]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape: torch.Size([1, 3, 196, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load image\n",
    "image = Image.open(\"checkboard_7.jpg\")\n",
    "\n",
    "# Resize and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Convert to patches (assuming 16x16 patch size)\n",
    "patch_size = 16\n",
    "patches = image_tensor.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "patches = patches.contiguous().view(1, 3, -1, patch_size, patch_size)  # Reshape to (B, C, N, P, P)\n",
    "\n",
    "print(\"Patches shape:\", patches.shape)  # (1, 3, 196, 16, 16) for 224x224 image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.4196,  0.3020,  0.2314,  ..., -0.5373, -0.5373, -0.5373],\n",
       "          [ 0.4196,  0.3098,  0.2471,  ..., -0.5373, -0.5294, -0.5294],\n",
       "          [ 0.4353,  0.3255,  0.2549,  ..., -0.5294, -0.5294, -0.5294],\n",
       "          ...,\n",
       "          [ 0.7804,  0.6314,  0.4353,  ..., -0.5529, -0.5529, -0.5608],\n",
       "          [ 0.7882,  0.6392,  0.4275,  ..., -0.5608, -0.5529, -0.5608],\n",
       "          [ 0.7961,  0.6471,  0.4196,  ..., -0.5608, -0.5608, -0.5608]],\n",
       "\n",
       "         [[ 0.4275,  0.3098,  0.2235,  ..., -0.5451, -0.5451, -0.5451],\n",
       "          [ 0.4353,  0.3176,  0.2392,  ..., -0.5451, -0.5373, -0.5373],\n",
       "          [ 0.4510,  0.3255,  0.2471,  ..., -0.5373, -0.5373, -0.5373],\n",
       "          ...,\n",
       "          [ 0.7412,  0.5922,  0.3961,  ..., -0.6314, -0.6314, -0.6314],\n",
       "          [ 0.7490,  0.6078,  0.3882,  ..., -0.6314, -0.6314, -0.6314],\n",
       "          [ 0.7647,  0.6157,  0.3804,  ..., -0.6314, -0.6314, -0.6314]],\n",
       "\n",
       "         [[ 0.4118,  0.2784,  0.1922,  ..., -0.7020, -0.7020, -0.7020],\n",
       "          [ 0.4196,  0.2863,  0.2078,  ..., -0.7020, -0.6941, -0.6863],\n",
       "          [ 0.4275,  0.2941,  0.2078,  ..., -0.6941, -0.6941, -0.6863],\n",
       "          ...,\n",
       "          [ 0.6863,  0.5451,  0.3490,  ..., -0.7647, -0.7569, -0.7569],\n",
       "          [ 0.6941,  0.5529,  0.3412,  ..., -0.7725, -0.7569, -0.7490],\n",
       "          [ 0.6941,  0.5608,  0.3333,  ..., -0.7725, -0.7647, -0.7569]]]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.7098, 0.6510, 0.6157,  ..., 0.5647, 0.5765, 0.5922],\n",
       "           [0.7098, 0.6549, 0.6235,  ..., 0.5647, 0.5804, 0.5922],\n",
       "           [0.7176, 0.6627, 0.6275,  ..., 0.5608, 0.5765, 0.5922],\n",
       "           ...,\n",
       "           [0.7686, 0.6941, 0.6392,  ..., 0.5529, 0.5647, 0.5804],\n",
       "           [0.7686, 0.6980, 0.6471,  ..., 0.5490, 0.5647, 0.5804],\n",
       "           [0.7765, 0.6980, 0.6471,  ..., 0.5490, 0.5647, 0.5725]],\n",
       "\n",
       "          [[0.6118, 0.6353, 0.6510,  ..., 0.7882, 0.7922, 0.7922],\n",
       "           [0.6078, 0.6314, 0.6510,  ..., 0.7882, 0.7922, 0.7922],\n",
       "           [0.6078, 0.6275, 0.6510,  ..., 0.7882, 0.7922, 0.7922],\n",
       "           ...,\n",
       "           [0.5961, 0.6118, 0.6353,  ..., 0.8000, 0.8000, 0.8078],\n",
       "           [0.5922, 0.6078, 0.6353,  ..., 0.7961, 0.8000, 0.8000],\n",
       "           [0.5882, 0.6078, 0.6353,  ..., 0.8000, 0.8039, 0.8000]],\n",
       "\n",
       "          [[0.7922, 0.7961, 0.7922,  ..., 0.7647, 0.7608, 0.7608],\n",
       "           [0.7922, 0.8000, 0.7961,  ..., 0.7647, 0.7608, 0.7608],\n",
       "           [0.7961, 0.8000, 0.7961,  ..., 0.7686, 0.7647, 0.7608],\n",
       "           ...,\n",
       "           [0.8039, 0.8039, 0.8039,  ..., 0.7725, 0.7686, 0.7647],\n",
       "           [0.8000, 0.8039, 0.8039,  ..., 0.7725, 0.7686, 0.7647],\n",
       "           [0.8000, 0.8039, 0.8078,  ..., 0.7765, 0.7725, 0.7686]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.1490, 0.1373, 0.1294,  ..., 0.0980, 0.0980, 0.0941],\n",
       "           [0.1412, 0.1294, 0.1216,  ..., 0.0863, 0.0863, 0.0902],\n",
       "           [0.1176, 0.1059, 0.0980,  ..., 0.1137, 0.1176, 0.1216],\n",
       "           ...,\n",
       "           [0.1608, 0.1608, 0.1608,  ..., 0.2863, 0.2784, 0.2745],\n",
       "           [0.2275, 0.2471, 0.2627,  ..., 0.2941, 0.2863, 0.2784],\n",
       "           [0.3176, 0.3216, 0.3216,  ..., 0.2980, 0.2902, 0.2784]],\n",
       "\n",
       "          [[0.0902, 0.0902, 0.0902,  ..., 0.1490, 0.1490, 0.1529],\n",
       "           [0.0980, 0.1020, 0.1098,  ..., 0.1490, 0.1490, 0.1529],\n",
       "           [0.1216, 0.1255, 0.1294,  ..., 0.1490, 0.1490, 0.1569],\n",
       "           ...,\n",
       "           [0.2667, 0.2627, 0.2549,  ..., 0.2549, 0.2471, 0.2392],\n",
       "           [0.2667, 0.2588, 0.2588,  ..., 0.2510, 0.2431, 0.2392],\n",
       "           [0.2667, 0.2627, 0.2627,  ..., 0.2471, 0.2431, 0.2431]],\n",
       "\n",
       "          [[0.1529, 0.1569, 0.1569,  ..., 0.1569, 0.1608, 0.1608],\n",
       "           [0.1529, 0.1529, 0.1569,  ..., 0.1569, 0.1608, 0.1569],\n",
       "           [0.1529, 0.1529, 0.1569,  ..., 0.1569, 0.1569, 0.1569],\n",
       "           ...,\n",
       "           [0.2392, 0.2392, 0.2431,  ..., 0.2235, 0.2235, 0.2196],\n",
       "           [0.2392, 0.2392, 0.2392,  ..., 0.2196, 0.2235, 0.2196],\n",
       "           [0.2353, 0.2353, 0.2392,  ..., 0.2196, 0.2196, 0.2196]]],\n",
       "\n",
       "\n",
       "         [[[0.7137, 0.6549, 0.6118,  ..., 0.4745, 0.4863, 0.4980],\n",
       "           [0.7176, 0.6588, 0.6196,  ..., 0.4745, 0.4863, 0.4980],\n",
       "           [0.7255, 0.6627, 0.6235,  ..., 0.4745, 0.4863, 0.4980],\n",
       "           ...,\n",
       "           [0.7843, 0.7059, 0.6431,  ..., 0.4588, 0.4706, 0.4824],\n",
       "           [0.7882, 0.7059, 0.6510,  ..., 0.4588, 0.4706, 0.4824],\n",
       "           [0.7922, 0.7059, 0.6510,  ..., 0.4549, 0.4667, 0.4784]],\n",
       "\n",
       "          [[0.5137, 0.5333, 0.5490,  ..., 0.6706, 0.6745, 0.6745],\n",
       "           [0.5137, 0.5333, 0.5529,  ..., 0.6706, 0.6745, 0.6745],\n",
       "           [0.5137, 0.5333, 0.5529,  ..., 0.6706, 0.6745, 0.6745],\n",
       "           ...,\n",
       "           [0.5020, 0.5216, 0.5412,  ..., 0.6824, 0.6824, 0.6784],\n",
       "           [0.4980, 0.5176, 0.5451,  ..., 0.6824, 0.6824, 0.6824],\n",
       "           [0.4941, 0.5216, 0.5451,  ..., 0.6824, 0.6863, 0.6824]],\n",
       "\n",
       "          [[0.6706, 0.6667, 0.6667,  ..., 0.6353, 0.6314, 0.6275],\n",
       "           [0.6706, 0.6706, 0.6706,  ..., 0.6353, 0.6353, 0.6275],\n",
       "           [0.6745, 0.6706, 0.6706,  ..., 0.6392, 0.6353, 0.6275],\n",
       "           ...,\n",
       "           [0.6824, 0.6784, 0.6745,  ..., 0.6471, 0.6431, 0.6353],\n",
       "           [0.6824, 0.6784, 0.6784,  ..., 0.6471, 0.6431, 0.6392],\n",
       "           [0.6824, 0.6784, 0.6784,  ..., 0.6510, 0.6471, 0.6392]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.0941, 0.0902, 0.0902,  ..., 0.0863, 0.0824, 0.0824],\n",
       "           [0.0863, 0.0824, 0.0863,  ..., 0.0745, 0.0784, 0.0784],\n",
       "           [0.0784, 0.0706, 0.0706,  ..., 0.0980, 0.1059, 0.1098],\n",
       "           ...,\n",
       "           [0.1059, 0.1059, 0.1098,  ..., 0.1961, 0.1804, 0.1725],\n",
       "           [0.1490, 0.1647, 0.1804,  ..., 0.1725, 0.1686, 0.1608],\n",
       "           [0.2039, 0.2000, 0.1961,  ..., 0.1725, 0.1647, 0.1569]],\n",
       "\n",
       "          [[0.0784, 0.0784, 0.0784,  ..., 0.1412, 0.1412, 0.1451],\n",
       "           [0.0863, 0.0941, 0.0980,  ..., 0.1412, 0.1412, 0.1451],\n",
       "           [0.1098, 0.1137, 0.1137,  ..., 0.1412, 0.1412, 0.1451],\n",
       "           ...,\n",
       "           [0.1647, 0.1608, 0.1647,  ..., 0.1765, 0.1765, 0.1765],\n",
       "           [0.1529, 0.1490, 0.1529,  ..., 0.1765, 0.1765, 0.1725],\n",
       "           [0.1529, 0.1529, 0.1529,  ..., 0.1725, 0.1725, 0.1725]],\n",
       "\n",
       "          [[0.1451, 0.1451, 0.1451,  ..., 0.1490, 0.1490, 0.1490],\n",
       "           [0.1451, 0.1451, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
       "           [0.1451, 0.1451, 0.1490,  ..., 0.1490, 0.1490, 0.1451],\n",
       "           ...,\n",
       "           [0.1765, 0.1804, 0.1804,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.1725, 0.1804, 0.1725,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.1686, 0.1725, 0.1725,  ..., 0.1843, 0.1843, 0.1843]]],\n",
       "\n",
       "\n",
       "         [[[0.7059, 0.6392, 0.5961,  ..., 0.4000, 0.4039, 0.4078],\n",
       "           [0.7098, 0.6431, 0.6039,  ..., 0.3961, 0.4039, 0.4078],\n",
       "           [0.7137, 0.6471, 0.6039,  ..., 0.3922, 0.4039, 0.4078],\n",
       "           ...,\n",
       "           [0.7765, 0.6863, 0.6196,  ..., 0.3882, 0.3922, 0.3961],\n",
       "           [0.7804, 0.6902, 0.6275,  ..., 0.3882, 0.3922, 0.3961],\n",
       "           [0.7843, 0.6902, 0.6314,  ..., 0.3843, 0.3882, 0.3922]],\n",
       "\n",
       "          [[0.4235, 0.4431, 0.4549,  ..., 0.5373, 0.5412, 0.5412],\n",
       "           [0.4196, 0.4392, 0.4549,  ..., 0.5412, 0.5412, 0.5412],\n",
       "           [0.4196, 0.4353, 0.4510,  ..., 0.5373, 0.5412, 0.5412],\n",
       "           ...,\n",
       "           [0.4118, 0.4235, 0.4392,  ..., 0.5490, 0.5490, 0.5451],\n",
       "           [0.4078, 0.4235, 0.4431,  ..., 0.5490, 0.5490, 0.5451],\n",
       "           [0.4039, 0.4275, 0.4431,  ..., 0.5490, 0.5529, 0.5490]],\n",
       "\n",
       "          [[0.5373, 0.5333, 0.5373,  ..., 0.4941, 0.4902, 0.4902],\n",
       "           [0.5373, 0.5373, 0.5333,  ..., 0.4980, 0.4902, 0.4863],\n",
       "           [0.5412, 0.5412, 0.5373,  ..., 0.4980, 0.4902, 0.4863],\n",
       "           ...,\n",
       "           [0.5412, 0.5412, 0.5373,  ..., 0.4980, 0.4941, 0.4902],\n",
       "           [0.5412, 0.5412, 0.5412,  ..., 0.4980, 0.4941, 0.4902],\n",
       "           [0.5412, 0.5412, 0.5412,  ..., 0.5020, 0.4980, 0.4902]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.0510, 0.0510, 0.0510,  ..., 0.0510, 0.0471, 0.0471],\n",
       "           [0.0471, 0.0471, 0.0471,  ..., 0.0392, 0.0392, 0.0431],\n",
       "           [0.0353, 0.0353, 0.0314,  ..., 0.0627, 0.0667, 0.0706],\n",
       "           ...,\n",
       "           [0.0510, 0.0510, 0.0510,  ..., 0.1098, 0.0980, 0.0902],\n",
       "           [0.0784, 0.0902, 0.1059,  ..., 0.0902, 0.0863, 0.0784],\n",
       "           [0.1176, 0.1098, 0.1098,  ..., 0.0902, 0.0824, 0.0745]],\n",
       "\n",
       "          [[0.0431, 0.0392, 0.0431,  ..., 0.0941, 0.0941, 0.0941],\n",
       "           [0.0510, 0.0549, 0.0588,  ..., 0.0941, 0.0941, 0.0941],\n",
       "           [0.0706, 0.0745, 0.0745,  ..., 0.0941, 0.0902, 0.0902],\n",
       "           ...,\n",
       "           [0.0824, 0.0784, 0.0784,  ..., 0.0980, 0.0980, 0.0980],\n",
       "           [0.0706, 0.0667, 0.0706,  ..., 0.0902, 0.0941, 0.0941],\n",
       "           [0.0706, 0.0706, 0.0706,  ..., 0.0863, 0.0902, 0.0941]],\n",
       "\n",
       "          [[0.0941, 0.0941, 0.0941,  ..., 0.0980, 0.0980, 0.0980],\n",
       "           [0.0941, 0.0941, 0.0980,  ..., 0.0980, 0.0980, 0.0980],\n",
       "           [0.0941, 0.0941, 0.0980,  ..., 0.0980, 0.0980, 0.0980],\n",
       "           ...,\n",
       "           [0.0980, 0.1059, 0.1059,  ..., 0.1176, 0.1216, 0.1216],\n",
       "           [0.0941, 0.1020, 0.1020,  ..., 0.1137, 0.1216, 0.1255],\n",
       "           [0.0902, 0.0941, 0.0980,  ..., 0.1137, 0.1176, 0.1216]]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ViTModel(\n  (embeddings): ViTEmbeddings(\n    (patch_embeddings): ViTPatchEmbeddings(\n      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): ViTEncoder(\n    (layer): ModuleList(\n      (0): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (1): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (2): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (3): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (4): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (5): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (6): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (7): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (8): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (9): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (10): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (11): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (pooler): ViTPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n) argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpatches)\n",
      "\u001b[0;31mTypeError\u001b[0m: ViTModel(\n  (embeddings): ViTEmbeddings(\n    (patch_embeddings): ViTPatchEmbeddings(\n      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): ViTEncoder(\n    (layer): ModuleList(\n      (0): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (1): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (2): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (3): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (4): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (5): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (6): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (7): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (8): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (9): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (10): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (11): ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (pooler): ViTPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n) argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netvlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
